
write about:



test:



on hold:


to do:
    balanced over class, slides and cohorts
    new model training each time?


    model might be overfit on the slide level - possible check - if it accurately classify tiles from train slide that wasnt in training (also on patient level..)
    # rotating validation sets - train test reduce ect


    think again about lr - it worked well on high


    different filters schedulers
    rethink only positive class
    soft labels
    different FoVs
    different cohorts





    consider pretrain using all and train only using single cohort - in that way you don't increase the number of training sample - could be very interesting
    tune on single cohort

    move to other biomarks



runs -
other -


general -
train and test on another cohort
Billal label files - do same logic for STAD/UCEC? - Yossi said - not makes sense
CPTAC is publicly available
Wang et al. trained the network using an unsupervised
contrastive loss on data from TCGA and PAIP36 from multiple organs and provided the weights for public use. The embeddings
for each tile are stored for the subsequent training procedure

understand the FoVs - some slides are big with low number of tiles (different Fovs?), does the cell oval size is the same size in all images? (or other common something)
some of the slide has weird tiling - single tile for something that look normal.. (not in labels..)







thinking:
look at the cleaning phases of Cell and the paper about UCEC


future:
3/5 CV - on all results before submission
testing on OOD dataset - could be a potential for improvement.
Using pretrained HIPT?
add more FoVs? like 384 and 786?


Questions:


comments:
    blur slides - not correlated to AUC
    slides mistakes  - looks pretty similar
    lightly?
    slide aware network?
    using positional encoding - not gonna work.