
write about:



test:



on hold:


to do:
    model might be overfit on the slide level, training slides all get super high AUC, check where the overfit in happenning
    possible splits:
    patient level - 2 slides of same patient - one in test, one in train
    slide level - random split of tiles of same slide - some in train and some in test
    location in slide level - split spatially the slide - 1 in test and the other in train



    tune each cohort on its on
    different FoVs
    lr
    ESCA
    fp


    # rotating validation sets - train test reduce ect






    different filters schedulers
    rethink only positive class
    soft labels

    pretrain using 2 streams and without ESCA



    consider pretrain using all and train only using single cohort - in that way you don't increase the number of training sample - could be very interesting
    tune on single cohort

    move to other biomarks



runs -
other -


general -
train and test on another cohort
Billal label files - do same logic for STAD/UCEC? - Yossi said - not makes sense
CPTAC is publicly available
Wang et al. trained the network using an unsupervised
contrastive loss on data from TCGA and PAIP36 from multiple organs and provided the weights for public use. The embeddings
for each tile are stored for the subsequent training procedure

understand the FoVs - some slides are big with low number of tiles (different Fovs?), does the cell oval size is the same size in all images? (or other common something)
some of the slide has weird tiling - single tile for something that look normal.. (not in labels..)







thinking:
look at the cleaning phases of Cell and the paper about UCEC


future:
3/5 CV - on all results before submission
testing on OOD dataset - could be a potential for improvement.
Using pretrained HIPT?
add more FoVs? like 384 and 786?


Questions:


comments:
    blur slides - not correlated to AUC
    slides mistakes  - looks pretty similar
    lightly?
    slide aware network?
    using positional encoding - not gonna work.