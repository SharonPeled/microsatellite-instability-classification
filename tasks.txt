
write about:



test:



on hold:
    run again on argus - to cancel overfit, try without symmetric loss (this time with), seed



to do:

    action items about variants - until next week - ready email

    train again tile based with lr adjustment and new files (different versions)
    check the awareness score of the adjusted model

    check awareness scores - per block, per head, per cohort, per entire image/patch, diff cq - adjust lr to the cq, weight dino_loss
    pretrain again, understand the params

    tune the FoVs rates

    consider pretrain using all and train only using single cohort - in that way you don't increase the number of training sample - could be very interesting
    tune on single cohort

    run experiment with that each epoch is reduced % tiles per slide
    run an experiment with random 512 tiles and mean agg
    save slides with tile embeddings and locations

    move to other biomarks



runs -
other -


general -
train and test on another cohort
Billal label files - do same logic for STAD/UCEC? - Yossi said - not makes sense
CPTAC is publicly available
Wang et al. trained the network using an unsupervised
contrastive loss on data from TCGA and PAIP36 from multiple organs and provided the weights for public use. The embeddings
for each tile are stored for the subsequent training procedure

understand the FoVs - some slides are big with low number of tiles (different Fovs?), does the cell oval size is the same size in all images? (or other common something)
some of the slide has weird tiling - single tile for something that look normal.. (not in labels..)







thinking:
look at the cleaning phases of Cell and the paper about UCEC


future:
3/5 CV - on all results before submission
testing on OOD dataset - could be a potential for improvement.
Using pretrained HIPT?
add more FoVs? like 384 and 786?


Questions:


comments:
    blur slides - not correlated to AUC
    slides mistakes  - looks pretty similar
    lightly?
    slide aware network?
    using positional encoding - not gonna work.